version: 2.1

# -------------------------------------------------------------------------------------
# Commands
# -------------------------------------------------------------------------------------

commands:

  py_3_7_setup:
    description: "Install and switch to Python 3.7.10; also install pip and pytest."
    steps:
      - run:
          name: "Setup Python v3.7.10 environment"
          command: |
            cd /opt/circleci/.pyenv && git pull && cd -
            pyenv install -s 3.7.10
            pyenv global 3.7.10
            pyenv local 3.7.10
            pyenv versions
            echo "In venv: $(pyenv local) - $(python -V), $(pip -V)"
            sudo "$(which python)" -m pip install --upgrade pip
            sudo "$(which python)" -m pip install pytest

  update_gpg_key:
    description: "Temp fix due to CircleCI image being old and requiring manual GPG key updates to Ubuntu, Nvidia, Heroku"
    steps:
      - run:
          name: "Update GPG keys"
          command: |
            wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
            curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
            distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
            curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
            sudo apt-get update
            curl https://cli-assets.heroku.com/apt/release.key | sudo apt-key add -

  run_nvidia_smi:
    description: "Prints GPU capabilities from nvidia-smi"
    steps:
      - run:
          name: "Run Nvidia-SMI"
          command: |
            nvidia-smi

  pip_dev_install:
    description: "Install dependencies via pip, including extra deps. Also supports more options, such as building on top of PyTorch nightly."
    parameters:
      args:
        type: string
        default: ""
    steps:
      - run:
          name: "Install dependencies via pip"
          command: ./scripts/install_via_pip.sh << parameters.args >>

  lint_flake8:
    description: "Lint with flake8"
    steps:
      - run:
          name: "Lint with flake8"
          command: flake8 --config ./.circleci/flake8_config.ini

  lint_black:
    description: "Lint with black"
    steps:
      - run:
          name: "Lint with black"
          command: black --check --diff --color .

  isort:
    description: "Check import order with isort"
    steps:
      - run:
          name: "Check import order with isort"
          command: isort -v -l 88 -o opacus --lines-after-imports 2 -m 3 --trailing-comma --check-only .

  mypy_check:
    description: "Static type checking with mypy"
    steps:
      - run:
          name: "Mypy checks"
          command: ./scripts/run_mypy.sh

  configure_docusaurus_bot:
    description: "Configure Docusaurus GitHub bot"
    steps:
      - run:
          name: "Configure Docusaurus GitHub bot"
          command: |
              git config --global user.email "docusaurus-bot@users.noreply.github.com"
              git config --global user.name "Opacus website deployment script"
              echo "machine github.com login docusaurus-bot password $DOCUSAURUS_GITHUB_TOKEN" > ~/.netrc

  deploy_site:
    description: "Deploy website to GitHub Pages"
    steps:
      - run:
          name: "Deploy website to GitHub Pages"
            # TODO: make the installation above conditional on there being relevant changes (no need to install if there are none)
          command: |
              mkdir -p website/static/.circleci && cp -a .circleci/. website/static/.circleci/.
              cd website
              ./scripts/build_website.sh -b
              GIT_USER=docusaurus-bot yarn run publish-gh-pages


  unit_tests:
    description: "Run unit tests"
    steps:
      - run:
          name: "Unit tests"
          no_output_timeout: 1h
          command: |
            mkdir unittest-reports
            python -m pytest --junitxml=unittest-reports/junit.xml

      - store_test_results:
          path: unittest-reports
      - store_artifacts:
          path: unittest-reports


  mnist_integration_test:
    description: "Runs MNIST example end to end"
    parameters:
      device:
        default: "cpu"
        type: string
    steps:
      - run:
          name: MNIST example
          command: |
            mkdir mnist
            mkdir mnist/data
            mkdir mnist/test-reports
            echo "Using $(python -V) ($(which python))"
            echo "Using $(pip -V) ($(which pip))"
            python examples/mnist.py --lr 0.25 --sigma 0.7 -c 1.5 --sample-rate 0.004 --epochs 1 --data-root mnist/data --n-runs 1 --device <<parameters.device>>
          when: always
      - store_test_results:
          path: mnist/test-reports
      - store_artifacts:
          path: mnist/test-reports

  cifar10_integration_test:
    description: "Runs CIFAR10 example end to end"
    parameters:
      device:
        default: "cpu"
        type: string
    steps:
      - run:
          name: CIFAR10 example
          command: |
            mkdir cifar10
            mkdir cifar10/data
            mkdir cifar10/logs
            mkdir cifar10/test-reports
            echo "Using $(python -V) ($(which python))"
            echo "Using $(pip -V) ($(which pip))"
            pip install tensorboard
            python examples/cifar10.py --lr 0.25 --sigma 0.7 -c 1.5 --sample-rate 0.001 --epochs 1 --data-root cifar10/data --log-dir cifar10/logs --device <<parameters.device>>
          when: always
      - store_test_results:
          path: cifar10/test-reports
      - store_artifacts:
          path: cifar10/test-reports

  dcgan_integration_test:
    description: "Runs dcgan example end to end"
    parameters:
      device:
        default: "cpu"
        type: string
    steps:
      - run:
          name: dcgan example
          command: |
            mkdir dcgan
            mkdir dcgan/data
            mkdir dcgan/test-reports
            echo "Using $(python -V) ($(which python))"
            echo "Using $(pip -V) ($(which pip))"
            python examples/dcgan.py --lr 2e-4 --sigma 0.7 -c 1.5 --sample-rate 0.01 --epochs 1 --data-root dcgan/data --device <<parameters.device>>
          when: always
      - store_test_results:
          path: dcgan/test-reports
      - store_artifacts:
          path: dcgan/test-reports

  imdb_integration_test:
    description: "Runs imdb example end to end"
    parameters:
      device:
        default: "cpu"
        type: string
    steps:
      - run:
          name: imdb example
          command: |
            mkdir imdb
            mkdir imdb/data
            mkdir imdb/test-reports
            echo "Using $(python -V) ($(which python))"
            echo "Using $(pip -V) ($(which pip))"
            pip install --user datasets transformers
            python examples/imdb.py --lr 0.02 --sigma 0.56 -c 1.0 --sample-rate 0.00256 --max-sequence-length 256 --epochs 1 --data-root imdb/data --device <<parameters.device>>
          when: always
      - store_test_results:
          path: imdb/test-reports
      - store_artifacts:
          path: imdb/test-reports

  charlstm_integration_test:
    description: "Runs charlstm example end to end"
    parameters:
      device:
        default: "cpu"
        type: string
    steps:
      - run:
          name: charlstm example
          command: |
            mkdir charlstm
            mkdir charlstm/data
            wget https://download.pytorch.org/tutorial/data.zip -O charlstm/data/data.zip
            unzip charlstm/data/data.zip -d charlstm/data
            rm charlstm/data/data.zip
            mkdir charlstm/test-reports
            echo "Using $(python -V) ($(which python))"
            echo "Using $(pip -V) ($(which pip))"
            pip install scikit-learn
            python examples/char-lstm-classification.py --epochs=20 --learning-rate=2.0 --hidden-size=128 --delta=8e-5 --sample-rate=0.05 --n-lstm-layers=1 --sigma=1.0 --max-per-sample-grad-norm=1.5 --data-root="charlstm/data/data/names/" --device=<<parameters.device>> --test-every 5
          when: always
      - store_test_results:
          path: charlstm/test-reports
      - store_artifacts:
          path: charlstm/test-reports

# -------------------------------------------------------------------------------------
# Jobs
# -------------------------------------------------------------------------------------

jobs:

  lint_py37_torch_release:
    docker:
      - image: cimg/python:3.7
    steps:
      - checkout
      - pip_dev_install
      - lint_flake8
      - lint_black
      - isort
      # - mypy_check  TODO re-enable

  unittest_py37_torch_release:
    docker:
      - image: cimg/python:3.7
    steps:
      - checkout
      - pip_dev_install
      - unit_tests

  unittest_py38_torch_release:
    docker:
      - image: cimg/python:3.8
    steps:
      - checkout
      - pip_dev_install
      - unit_tests

  unittest_py39_torch_release:
    docker:
      - image: cimg/python:3.9
    steps:
      - checkout
      - pip_dev_install
      - unit_tests

  unittest_py39_torch_nightly:
    docker:
      - image: cimg/python:3.9
    steps:
      - checkout
      - pip_dev_install:
          args: "-n"
      - unit_tests

  integrationtest_py37_torch_release_cpu:
    docker:
      - image: cimg/python:3.7
    steps:
      - checkout
      - pip_dev_install
      - mnist_integration_test:
          device: "cpu"

  integrationtest_py37_torch_release_cuda:
    machine:
      resource_class: gpu.nvidia.small
      image: ubuntu-1604-cuda-11.1:202012-01
    steps:
      - update_gpg_key
      - checkout
      - py_3_7_setup
      - pip_dev_install
      - run_nvidia_smi
      - mnist_integration_test:
          device: "cuda"
      - cifar10_integration_test:
          device: "cuda"
      - imdb_integration_test:
          device: "cuda"
      - charlstm_integration_test:
          device: "cuda"
      - dcgan_integration_test:
          device: "cuda"

  unittest_multi_gpu:
    machine:
      resource_class: gpu.medium
      image: ubuntu-1604-cuda-11.1:202012-01
    steps:
      - update_gpg_key
      - checkout
      - py_3_7_setup
      - pip_dev_install
      - run_nvidia_smi
      - run:
          name: "Unit test multi_gpu"
          no_output_timeout: 1h
          command: |
            mkdir unittest-multigpu-reports
            python -m unittest opacus.tests.multigpu_gradcheck.GradientComputationTest.test_gradient_correct


  auto_deploy_site:
    docker:
      - image: cimg/python:3.9-node
    steps:
      - run: node --version
      - run: yarn --version
      - checkout
      - pip_dev_install:
          args: "-n -d"
      - configure_docusaurus_bot
      - deploy_site


aliases:

  - &exclude_ghpages
    branches:
      ignore:
        - gh-pages

# -------------------------------------------------------------------------------------
# Workflows
# -------------------------------------------------------------------------------------

workflows:
  commit:
    jobs:
      - lint_py37_torch_release:
          filters: *exclude_ghpages
      - unittest_py37_torch_release:
          filters: *exclude_ghpages
      - unittest_py38_torch_release:
          filters: *exclude_ghpages
      - unittest_py39_torch_release:
          filters: *exclude_ghpages
      - unittest_py39_torch_nightly:
          filters: *exclude_ghpages
      - unittest_multi_gpu:
          filters: *exclude_ghpages
      - integrationtest_py37_torch_release_cpu:
          filters: *exclude_ghpages
      - integrationtest_py37_torch_release_cuda:
          filters: *exclude_ghpages

  nightly:
    triggers:
      - schedule:
          cron: "0 0 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - unittest_py39_torch_nightly:
          filters: *exclude_ghpages
      - integrationtest_py37_torch_release_cpu:
          filters: *exclude_ghpages
      - integrationtest_py37_torch_release_cuda:
          filters: *exclude_ghpages

  website_deployment:
    jobs:
      - auto_deploy_site:
          filters:
            branches:
              only:
                - master
