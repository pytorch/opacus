<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Opacus · Train PyTorch models with Differential Privacy</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Train PyTorch models with Differential Privacy"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Opacus · Train PyTorch models with Differential Privacy"/><meta property="og:type" content="website"/><meta property="og:url" content="https://opacus.ai/"/><meta property="og:description" content="Train PyTorch models with Differential Privacy"/><meta property="og:image" content="https://opacus.ai/img/opacus_logo.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://opacus.ai/img/opacus_logo.svg"/><link rel="shortcut icon" href="/img/opacus_favicon.svg"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-117752657-3', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="/css/code_block_buttons.css"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="/js/code_block_buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/mathjax.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_HTML"></script><script src="https://unpkg.com/vanilla-back-to-top@7.1.14/dist/vanilla-back-to-top.min.js"></script><script>
        document.addEventListener('DOMContentLoaded', function() {
          addBackToTop(
            {"zIndex":100}
          )
        });
        </script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/opacus_logo.svg" alt="Opacus"/></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/introduction" target="_self">Introduction</a></li><li class=""><a href="/docs/faq" target="_self">FAQ</a></li><li class=""><a href="/tutorials/" target="_self">Tutorials</a></li><li class=""><a href="/api/" target="_self">API Reference</a></li><li class=""><a href="https://github.com/pytorch/opacus" target="_self">GitHub</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div>
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<section id="dpoptimizerfastgradientclipping">
<h1>DPOptimizerFastGradientClipping<a class="headerlink" href="#dpoptimizerfastgradientclipping" title="Link to this heading">¶</a></h1>
<dl class="py class" id="module-opacus.optimizers.optimizer_fast_gradient_clipping">
<dt class="sig sig-object py" id="opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">opacus.optimizers.optimizer_fast_gradient_clipping.</span></span><span class="sig-name descname"><span class="pre">DPOptimizerFastGradientClipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">noise_multiplier</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_grad_norm</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">expected_batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">secure_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/opacus/optimizers/optimizer_fast_gradient_clipping.html#DPOptimizerFastGradientClipping"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping" title="Link to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> wrapper to implement Fast Gradient and Ghost Clipping – modifies DPOptimizer
to only add noise to the average gradient, without clipping.</p>
<p>Can be used with any <code class="docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> subclass as an underlying optimizer.
<code class="docutils literal notranslate"><span class="pre">DPOptimizerFastGradientClipping</span></code> assumes that parameters over which it performs optimization belong
to GradSampleModuleFastGradientClipping and therefore have the <code class="docutils literal notranslate"><span class="pre">grad_sample</span></code> attribute.</p>
<p>On a high level <code class="docutils literal notranslate"><span class="pre">DPOptimizerFastGradientClipping</span></code>’s step looks like this:
1) Add Gaussian noise to <code class="docutils literal notranslate"><span class="pre">p.grad</span></code> calibrated to a given noise multiplier and
max grad norm limit (<code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">=</span> <span class="pre">noise_multiplier</span> <span class="pre">*</span> <span class="pre">max_grad_norm</span></code>).
2) Call underlying optimizer to perform optimization step</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">MyCustomModel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dp_optimizer</span> <span class="o">=</span> <span class="n">DPOptimizerFastGradientClipping</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">noise_multiplier</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_grad_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">expected_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Optimizer</span></code></a></span>) – wrapped optimizer.</p></li>
<li><p><strong>noise_multiplier</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span>) – noise multiplier</p></li>
<li><p><strong>max_grad_norm</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a></span>) – max grad norm used for calculating the standard devition of noise added</p></li>
<li><p><strong>expected_batch_size</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></a>]</span>) – batch_size used for averaging gradients. When using
Poisson sampling averaging denominator can’t be inferred from the
actual batch size. Required is <code class="docutils literal notranslate"><span class="pre">loss_reduction="mean"</span></code>, ignored if
<code class="docutils literal notranslate"><span class="pre">loss_reduction="sum"</span></code></p></li>
<li><p><strong>loss_reduction</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></a></span>) – Indicates if the loss reduction (for aggregating the gradients)
is a sum or a mean operation. Can take values “sum” or “mean”</p></li>
<li><p><strong>generator</strong> – torch.Generator() object used as a source of randomness for
the noise</p></li>
<li><p><strong>secure_mode</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a></span>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code> uses noise generation approach robust to floating
point arithmetic attacks.
See <code class="xref py py-meth docutils literal notranslate"><span class="pre">_generate_noise()</span></code> for details</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.accumulate">
<span class="sig-name descname"><span class="pre">accumulate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/opacus/optimizers/optimizer_fast_gradient_clipping.html#DPOptimizerFastGradientClipping.accumulate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.accumulate" title="Link to this definition">¶</a></dt>
<dd><p>Performs gradient accumulation.
Stores aggregated gradients into <cite>p.summed_grad``</cite></p>
</dd></dl>
<dl class="py property">
<dt class="sig sig-object py" id="opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.accumulated_iterations">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">accumulated_iterations</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></em><a class="headerlink" href="#opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.accumulated_iterations" title="Link to this definition">¶</a></dt>
<dd><p>Returns number of batches currently accumulated and not yet processed.</p>
<p>In other words <code class="docutils literal notranslate"><span class="pre">accumulated_iterations</span></code> tracks the number of forward/backward
passed done in between two optimizer steps. The value would typically be 1,
but there are possible exceptions.</p>
<p>Used by privacy accountants to calculate real sampling rate.</p>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.clip_and_accumulate">
<span class="sig-name descname"><span class="pre">clip_and_accumulate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/opacus/optimizers/optimizer_fast_gradient_clipping.html#DPOptimizerFastGradientClipping.clip_and_accumulate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.clip_and_accumulate" title="Link to this definition">¶</a></dt>
<dd><p>Redefines a parent class’ function to not do anything</p>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.pre_step">
<span class="sig-name descname"><span class="pre">pre_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">closure</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/opacus/optimizers/optimizer_fast_gradient_clipping.html#DPOptimizerFastGradientClipping.pre_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.pre_step" title="Link to this definition">¶</a></dt>
<dd><p>Perform actions specific to <code class="docutils literal notranslate"><span class="pre">DPOptimizer</span></code> before calling
underlying  <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>closure</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.13)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code></a>[[], <a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]]</span>) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.13)"><code class="xref py py-data docutils literal notranslate"><span class="pre">Optional</span></code></a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></a>]</span></p>
</dd>
</dl>
</dd></dl>
<dl class="py method">
<dt class="sig sig-object py" id="opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/opacus/optimizers/optimizer_fast_gradient_clipping.html#DPOptimizerFastGradientClipping.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#opacus.optimizers.optimizer_fast_gradient_clipping.DPOptimizerFastGradientClipping.zero_grad" title="Link to this definition">¶</a></dt>
<dd><p>Clear gradients.</p>
<p>Clears <code class="docutils literal notranslate"><span class="pre">p.grad</span></code> and <code class="docutils literal notranslate"><span class="pre">p.summed_grad</span></code> for all of it’s parameters</p>
<p class="rubric">Notes</p>
<p><code class="docutils literal notranslate"><span class="pre">set_to_none</span></code> argument only affects <code class="docutils literal notranslate"><span class="pre">p.grad</span></code> and
<code class="docutils literal notranslate"><span class="pre">p.summed_grad</span></code> is never zeroed out and always set to None.
Normal grads can do this, because their shape is always the same.
Grad samples do not behave like this, as we accumulate gradients from different
batches in a list</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>set_to_none</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></a></span>) – instead of setting to zero, set the grads to None. (only</p></li>
<li><p><strong>None</strong><strong>)</strong> (<em>affects regular gradients. Per sample gradients are always set to</em>)</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</dd></dl>
</section>
</div>
</div>
</div>
<div aria-label="Main" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Opacus</a></h1>
<search id="searchbox" role="search" style="display: none">
<div class="searchformwrapper">
<form action="../search.html" class="search" method="get">
<input aria-labelledby="searchlabel" autocapitalize="off" autocomplete="off" autocorrect="off" name="q" placeholder="Search" spellcheck="false" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../privacy_engine.html">Privacy Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../grad_sample_module.html">GradSampleModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../grad_sample_module_fast_gradient_clipping.html">GradSampleModuleFastGradientClipping</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="optimizers.html">Optimizers</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="dp_optimizer.html">DPOptimizer</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">DPOptimizerFastGradientClipping</a></li>
<li class="toctree-l2"><a class="reference internal" href="dp_per_layer_optimizer.html">DPPerLayerOptimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="dp_ddp_optimizer.html">DistributedDPOptimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="dp_ddp_optimizer_fast_gradient_clipping.html">DistributedDPOptimizerFastGradientClipping</a></li>
<li class="toctree-l2"><a class="reference internal" href="dp_ddp_per_layer_optimizer.html">DistributedPerLayerOptimizer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../data_loader.html">DP Data Loader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../accounting/accounting.html">Privacy Accounting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../validator.html">ModuleValidator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.html">Distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../noise_scheduler.html">Noise Scheduler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../batch_memory_manager.html">Batch Memory Manager</a></li>
<li class="toctree-l1"><a class="reference internal" href="../layers.html">DP Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/utils.html">Utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../scripts.html">Scripts</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="../index.html">Documentation overview</a><ul>
<li><a href="optimizers.html">Optimizers</a><ul>
<li>Previous: <a href="dp_optimizer.html" title="previous chapter">DPOptimizer</a></li>
<li>Next: <a href="dp_per_layer_optimizer.html" title="next chapter">DPPerLayerOptimizer</a></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="clearer"></div>
</div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/" class="nav-home"><img src="/img/opacus_favicon.svg" alt="Opacus" width="66" height="58"/></a><div class="footerSection"><h5>Docs</h5><a href="/docs/introduction">Introduction</a><a href="/docs/faq">FAQ</a><a href="/tutorials/">Tutorials</a><a href="/api/">API Reference</a></div><div class="footerSection"><h5>Github</h5><div class="social"><a class="github-button" href="https://github.com/pytorch/opacus" data-count-href="https://github.com/pytorch/opacus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star Opacus on GitHub">opacus</a></div></div><div class="footerSection"><h5>Legal</h5><a href="https://opensource.facebook.com/legal/privacy/" target="_blank" rel="noreferrer noopener">Privacy</a><a href="https://opensource.facebook.com/legal/terms/" target="_blank" rel="noreferrer noopener">Terms</a></div></section><a href="https://opensource.facebook.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/oss_logo.png" alt="Meta Open Source" width="250" height="95"/></a><section class="copyright"> Copyright © 2025 Meta Platforms, Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '207c27d819f967749142d8611de7cb19',
                indexName: 'opacus',
                inputSelector: '#search_input_react'
              });
            </script></body></html>